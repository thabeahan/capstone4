{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Fine-Tune YOLOv12 for Rock-Paper-Scissors Detection\n","\n","This notebook demonstrates how to fine-tune the YOLOv12 object detection model on a custom dataset of rock-paper-scissors hand gestures. We'll go through environment setup, training, evaluation, and inference steps."],"metadata":{"id":"JwdYnZN6Q7fi"}},{"cell_type":"markdown","source":["## Environment setup"],"metadata":{"id":"mDLb-PJSkcj2"}},{"cell_type":"markdown","source":["### Step 1: Install Dependencies\n","\n","We install the required packages:\n","- `ultralytics`: Official package to use YOLO models including the new YOLOv12.\n","- `supervision`: A toolset for evaluating and visualizing object detection results."],"metadata":{"id":"_4_N105jkojb"}},{"cell_type":"code","source":["!pip install -qU ultralytics supervision"],"metadata":{"collapsed":true,"id":"xZ-4SmqAXM0x","executionInfo":{"status":"ok","timestamp":1763073358774,"user_tz":-420,"elapsed":7523,"user":{"displayName":"Tmabunda Hahabe","userId":"14655309613109040543"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"99f63ab1-f7b3-4197-fec2-0d3cb08ec664"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.6/1.1 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/207.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.2/207.2 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"markdown","source":["### Step 2: Check GPU Availability\n","\n","**NOTE:** **YOLOv12 leverages FlashAttention to speed up attention-based computations, but this feature requires an Nvidia GPU built on the Ampere architecture or newerâ€”for example, GPUs like the RTX 3090, RTX 3080, or even the Nvidia L4 meet this requirement.**\n","\n","Let's make sure that we have access to GPU. We can use `nvidia-smi` command to do that. In case of any problems navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `GPU`, and then click `Save`."],"metadata":{"id":"IOu5sknikgHP"}},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fcobt80KXqXg","executionInfo":{"status":"ok","timestamp":1763073358844,"user_tz":-420,"elapsed":62,"user":{"displayName":"Tmabunda Hahabe","userId":"14655309613109040543"}},"outputId":"1e3531d9-83b4-463d-9466-3e2456847488"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Thu Nov 13 22:35:58 2025       \n","+-----------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n","|-----------------------------------------+------------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                        |               MIG M. |\n","|=========================================+========================+======================|\n","|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n","| N/A   47C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n","|                                         |                        |                  N/A |\n","+-----------------------------------------+------------------------+----------------------+\n","                                                                                         \n","+-----------------------------------------------------------------------------------------+\n","| Processes:                                                                              |\n","|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n","|        ID   ID                                                               Usage      |\n","|=========================================================================================|\n","|  No running processes found                                                             |\n","+-----------------------------------------------------------------------------------------+\n"]}]},{"cell_type":"markdown","source":["### Step 3: Set Working Directory\n","\n","This sets up the current working directory and store in a variable for use later when accessing files or saving models, and prints it for confirmation."],"metadata":{"id":"9dUvGP6qRm1-"}},{"cell_type":"code","source":["import os\n","import locale\n","import supervision as sv\n","from IPython.display import Image\n","from ultralytics import YOLO\n","from google.colab import drive\n","from supervision import Detections\n","from supervision.metrics import MeanAveragePrecision\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","HOME = os.getcwd()\n","print(HOME)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pMG8pMVWiYpE","executionInfo":{"status":"ok","timestamp":1763073394642,"user_tz":-420,"elapsed":28242,"user":{"displayName":"Tmabunda Hahabe","userId":"14655309613109040543"}},"outputId":"ae498572-ea28-4a50-df4b-f9ad2d05b452"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Creating new Ultralytics Settings v0.0.6 file âœ… \n","View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n","Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n","Mounted at /content/drive\n","/content\n"]}]},{"cell_type":"code","source":["dataset_location = \"/content/drive/MyDrive/Capstone4\""],"metadata":{"id":"sggyk-rEVVnM","executionInfo":{"status":"ok","timestamp":1763073394643,"user_tz":-420,"elapsed":3,"user":{"displayName":"Tmabunda Hahabe","userId":"14655309613109040543"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["!ls {dataset_location}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DC68yjujVHvm","executionInfo":{"status":"ok","timestamp":1763073395026,"user_tz":-420,"elapsed":384,"user":{"displayName":"Tmabunda Hahabe","userId":"14655309613109040543"}},"outputId":"8130581f-57dc-4f8f-8efe-4669b502b56d"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["data.yaml\t     Model_YOLOv12V0.ipynb   test\t YOLO_Backups\n","data.zip\t     README.dataset.txt      train\n","Extract_Zip.ipynb    README.roboflow.txt     train1.zip\n","Model_YOLOv12.ipynb  split_test_valid.ipynb  valid\n"]}]},{"cell_type":"code","source":["!cat {dataset_location}/data.yaml"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kdR-UJ7wVwK7","executionInfo":{"status":"ok","timestamp":1763073395538,"user_tz":-420,"elapsed":497,"user":{"displayName":"Tmabunda Hahabe","userId":"14655309613109040543"}},"outputId":"573585d3-ddea-4e90-c77c-04d19587d9be"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["train: ../train/images\n","val: ../valid/images\n","test: ../test/images\n","\n","nc: 3\n","names: ['bus', 'car', 'van']\n","\n","roboflow:\n","  workspace: personal-project-kej16\n","  project: vehicle-detection-vznzd-dkl8g\n","  version: 1\n","  license: CC BY 4.0\n","  url: https://universe.roboflow.com/personal-project-kej16/vehicle-detection-vznzd-dkl8g/dataset/1"]}]},{"cell_type":"markdown","source":["### Step 6: Download Pretrained YOLOv12 Weights\n","\n","Downloads a pretrained YOLOv12 nano model (`yolov12n.pt`) to be fine-tuned."],"metadata":{"id":"Txx68yloR53l"}},{"cell_type":"code","source":["!wget https://github.com/sunsmarterjie/yolov12/releases/download/v1.0/yolov12n.pt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u3zHOAdoY93g","executionInfo":{"status":"ok","timestamp":1762948456525,"user_tz":-420,"elapsed":208,"user":{"displayName":"Tmabunda Hahabe","userId":"14655309613109040543"}},"outputId":"c144cd65-6788-4474-a45f-ec1fd4360aa9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-11-12 11:54:15--  https://github.com/sunsmarterjie/yolov12/releases/download/v1.0/yolov12n.pt\n","Resolving github.com (github.com)... 140.82.112.3\n","Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://release-assets.githubusercontent.com/github-production-release-asset/928546208/99db71db-0946-4c10-94a0-b54baf471037?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-11-12T12%3A46%3A05Z&rscd=attachment%3B+filename%3Dyolov12n.pt&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-11-12T11%3A45%3A21Z&ske=2025-11-12T12%3A46%3A05Z&sks=b&skv=2018-11-09&sig=9Usj2YT%2FBC6szXpP4XeZoPrJRR76jV3hxpqYhn4on2k%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2Mjk0ODc1NiwibmJmIjoxNzYyOTQ4NDU2LCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.TUZKbJz8mfpEjUAmFR69y9NgxbKZIeb6EBBLrOnapVg&response-content-disposition=attachment%3B%20filename%3Dyolov12n.pt&response-content-type=application%2Foctet-stream [following]\n","--2025-11-12 11:54:16--  https://release-assets.githubusercontent.com/github-production-release-asset/928546208/99db71db-0946-4c10-94a0-b54baf471037?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-11-12T12%3A46%3A05Z&rscd=attachment%3B+filename%3Dyolov12n.pt&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-11-12T11%3A45%3A21Z&ske=2025-11-12T12%3A46%3A05Z&sks=b&skv=2018-11-09&sig=9Usj2YT%2FBC6szXpP4XeZoPrJRR76jV3hxpqYhn4on2k%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2Mjk0ODc1NiwibmJmIjoxNzYyOTQ4NDU2LCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.TUZKbJz8mfpEjUAmFR69y9NgxbKZIeb6EBBLrOnapVg&response-content-disposition=attachment%3B%20filename%3Dyolov12n.pt&response-content-type=application%2Foctet-stream\n","Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 5595063 (5.3M) [application/octet-stream]\n","Saving to: â€˜yolov12n.ptâ€™\n","\n","\ryolov12n.pt           0%[                    ]       0  --.-KB/s               \ryolov12n.pt         100%[===================>]   5.33M  --.-KB/s    in 0.02s   \n","\n","2025-11-12 11:54:16 (284 MB/s) - â€˜yolov12n.ptâ€™ saved [5595063/5595063]\n","\n"]}]},{"cell_type":"markdown","source":["### Step 7: Load Pretrained YOLO Model\n","\n","Initializes the YOLO model using the downloaded weights."],"metadata":{"id":"pKlU83ELlKN6"}},{"cell_type":"code","source":["model = YOLO('/content/yolov12n.pt')"],"metadata":{"id":"8JTAaAfoSB6k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Step 8: Train the Model\n","\n","Fine-tune YOLOv12 using our dataset for 20 epochs.\n","Parameters include:\n","- `data`: path to YAML config\n","- `epochs`: total number of training epochs\n","- `batch`: number of images per batch\n","- `imgsz`: input image size\n","- `patience`: early stopping if no improvement\n","- `save_period`: save model checkpoint every 5 epochs\n","- `val`: whether to validate\n","- `flipud`: probability of vertical flip augmentation\n","\n","Trains YOLOv12 model on your dataset and saves it under runs/detect/train.\n"],"metadata":{"id":"PoWwXmp_SD_F"}},{"cell_type":"code","source":["save_dir = \"/content/drive/MyDrive/Capstone4/YOLO_Backups\"\n","results = model.train(\n","    data=f'{dataset_location}/data.yaml',  # Path to dataset config\n","    epochs=100,\n","    batch=16,\n","    imgsz=640,\n","    project=save_dir,         # Save all results here\n","    name='run1',              # Folder name inside /yolo_backups (e.g. yolo_backups/run1)\n","    exist_ok=True,            # Allow overwriting existing run folder\n","    patience=5,               # Early stopping\n","    save_period=3,            # Save every 3 epochs\n","    val=True,                 # Validate during training\n","    verbose=True,             # Show details\n","    fliplr=0.5,               # Data augmentation (horizontal flip)\n","    scale=0.5\n",")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3e6TLXsbnaE1","outputId":"458f443e-f1d6-4db8-b1bd-427603a3fcfb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Ultralytics 8.3.227 ğŸš€ Python-3.12.12 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n","\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/drive/MyDrive/Capstone4/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=100, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=/content/yolov12n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=run1, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=5, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=/content/drive/MyDrive/Capstone4/YOLO_Backups, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/drive/MyDrive/Capstone4/YOLO_Backups/run1, save_frames=False, save_json=False, save_period=3, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n","\u001b[KDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf': 100% â”â”â”â”â”â”â”â”â”â”â”â” 755.1KB 148.4MB/s 0.0s\n","Overriding model.yaml nc=80 with nc=3\n","\n","                   from  n    params  module                                       arguments                     \n","  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n","  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n","  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n","  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n","  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n","  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n","  6                  -1  2    180864  ultralytics.nn.modules.block.A2C2f           [128, 128, 2, True, 4]        \n","  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n","  8                  -1  2    689408  ultralytics.nn.modules.block.A2C2f           [256, 256, 2, True, 1]        \n","  9                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 10             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 11                  -1  1     86912  ultralytics.nn.modules.block.A2C2f           [384, 128, 1, False, -1]      \n"," 12                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 13             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 14                  -1  1     24000  ultralytics.nn.modules.block.A2C2f           [256, 64, 1, False, -1]       \n"," 15                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n"," 16            [-1, 11]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 17                  -1  1     74624  ultralytics.nn.modules.block.A2C2f           [192, 128, 1, False, -1]      \n"," 18                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n"," 19             [-1, 8]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 20                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n"," 21        [14, 17, 20]  1    431257  ultralytics.nn.modules.head.Detect           [3, [64, 128, 256]]           \n","YOLOv12n summary: 272 layers, 2,568,633 parameters, 2,568,617 gradients, 6.5 GFLOPs\n","\n","Transferred 640/691 items from pretrained weights\n","Freezing layer 'model.21.dfl.conv.weight'\n","\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n","\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt': 100% â”â”â”â”â”â”â”â”â”â”â”â” 5.4MB 283.4MB/s 0.0s\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n","\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 93.3Â±73.7 ms, read: 0.1Â±0.0 MB/s, size: 45.9 KB)\n","\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Capstone4/train/labels... 9218 images, 15 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 9218/9218 0.7it/s 3:41:15\n","\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/drive/MyDrive/Capstone4/train/labels.cache\n","WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 84, len(boxes) = 18494. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 1.0Â±1.0 ms, read: 0.0Â±0.0 MB/s, size: 28.5 KB)\n","\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Capstone4/valid/labels... 287 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 287/287 1.4it/s 3:30\n","\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/drive/MyDrive/Capstone4/valid/labels.cache\n","WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 19, len(boxes) = 427. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n","Plotting labels to /content/drive/MyDrive/Capstone4/YOLO_Backups/run1/labels.jpg... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01, momentum=0.9) with parameter groups 113 weight(decay=0.0), 120 weight(decay=0.0005), 119 bias(decay=0.0)\n","Image sizes 640 train, 640 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1m/content/drive/MyDrive/Capstone4/YOLO_Backups/run1\u001b[0m\n","Starting training for 100 epochs...\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      1/100       3.3G      0.856      1.809       1.13         30        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 577/577 2.3it/s 4:10\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 9/9 0.6it/s 15.4s\n","                   all        287        427        0.7      0.709      0.728      0.504\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      2/100       3.7G     0.8476      1.246      1.125          9        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 577/577 2.8it/s 3:26\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 9/9 2.9it/s 3.2s\n","                   all        287        427      0.608      0.665      0.684      0.481\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      3/100      3.72G     0.9312      1.233      1.187          7        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 577/577 2.8it/s 3:26\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 9/9 2.0it/s 4.5s\n","                   all        287        427      0.663      0.557      0.584      0.354\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      4/100      3.73G     0.9634      1.196      1.222          8        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 577/577 2.8it/s 3:26\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 9/9 2.4it/s 3.7s\n","                   all        287        427      0.774      0.642      0.734      0.492\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      5/100      3.74G     0.8997      1.066       1.19          4        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 577/577 2.8it/s 3:24\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 9/9 2.9it/s 3.1s\n","                   all        287        427      0.713      0.752      0.797      0.555\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      6/100      3.74G     0.8471      0.952      1.157         11        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 577/577 2.8it/s 3:23\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 9/9 2.8it/s 3.2s\n","                   all        287        427      0.641      0.735      0.782      0.549\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      7/100      3.76G     0.8156     0.8997      1.141          3        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 577/577 2.8it/s 3:26\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 9/9 3.0it/s 3.0s\n","                   all        287        427      0.739       0.76      0.783       0.56\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      8/100      3.77G     0.7888      0.839      1.121         10        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 577/577 2.8it/s 3:25\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 9/9 2.2it/s 4.1s\n","                   all        287        427      0.844      0.775       0.85      0.606\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      9/100      3.78G     0.7643     0.8036      1.106          7        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 577/577 2.8it/s 3:24\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 9/9 2.5it/s 3.6s\n","                   all        287        427      0.836      0.735      0.841      0.616\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     10/100      3.78G     0.7442     0.7813      1.096          4        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 577/577 2.8it/s 3:25\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 9/9 2.6it/s 3.5s\n","                   all        287        427      0.794       0.77      0.838      0.614\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     11/100       3.8G     0.7295      0.752      1.089         15        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 577/577 2.8it/s 3:28\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 9/9 2.8it/s 3.2s\n","                   all        287        427      0.776      0.731      0.797      0.577\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     12/100      3.81G     0.7116     0.7256      1.078          4        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 577/577 2.8it/s 3:28\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 9/9 2.1it/s 4.2s\n","                   all        287        427      0.751      0.797      0.844      0.616\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     13/100      3.82G      0.699     0.7135      1.074          9        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 577/577 2.8it/s 3:23\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 9/9 3.2it/s 2.8s\n","                   all        287        427      0.768      0.799      0.845      0.603\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     14/100      3.82G     0.6934     0.6945      1.067          5        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 577/577 2.8it/s 3:25\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 9/9 3.3it/s 2.8s\n","                   all        287        427      0.716      0.803      0.823      0.616\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     15/100      3.84G     0.6786     0.6731      1.061          6        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 577/577 2.8it/s 3:25\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 9/9 2.9it/s 3.1s\n","                   all        287        427      0.839      0.776      0.871      0.647\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     16/100      3.85G     0.6705     0.6607      1.055          6        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 577/577 2.8it/s 3:28\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 9/9 2.1it/s 4.3s\n","                   all        287        427      0.797       0.79      0.862      0.658\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     17/100      3.86G     0.6648     0.6487      1.052          9        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 577/577 2.8it/s 3:24\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 9/9 2.6it/s 3.5s\n","                   all        287        427      0.833      0.815      0.889      0.674\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     18/100      3.86G     0.6544     0.6355      1.044         11        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 577/577 2.8it/s 3:25\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 9/9 3.1it/s 2.9s\n","                   all        287        427      0.825      0.786      0.878      0.653\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     19/100      3.88G     0.6501     0.6234      1.044         14        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 577/577 2.9it/s 3:22\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 9/9 2.9it/s 3.1s\n","                   all        287        427      0.822       0.79       0.85      0.643\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     20/100      3.89G     0.6393     0.6067      1.038         11        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 577/577 2.8it/s 3:23\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 9/9 2.9it/s 3.1s\n","                   all        287        427      0.802      0.803      0.886      0.674\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     21/100       3.9G     0.6388     0.5991      1.037          8        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 577/577 2.8it/s 3:23\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 9/9 3.1it/s 2.9s\n","                   all        287        427      0.856      0.772      0.875      0.673\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     22/100       3.9G     0.6255     0.5916      1.028          6        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 577/577 2.8it/s 3:23\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 9/9 2.9it/s 3.1s\n","                   all        287        427      0.823       0.83      0.878      0.678\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     23/100      3.92G     0.6197     0.5816      1.026          7        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 577/577 2.8it/s 3:28\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 9/9 2.3it/s 4.0s\n","                   all        287        427      0.819      0.825      0.887      0.676\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     24/100      3.93G     0.6161     0.5699      1.025          5        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 577/577 2.8it/s 3:27\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 9/9 2.9it/s 3.1s\n","                   all        287        427      0.851      0.813      0.893      0.688\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     25/100      3.93G     0.6154     0.5616      1.018         59        640: 69% â”â”â”â”â”â”â”â”â”€â”€â”€â”€ 396/577 2.6it/s 2:25<1:09"]}]},{"cell_type":"markdown","source":["Continue Training"],"metadata":{"id":"6HlcnPEJGRg2"}},{"cell_type":"code","source":["model = YOLO('/content/drive/MyDrive/Capstone4/YOLO_Backups/run1/weights/last.pt')"],"metadata":{"id":"dUcYFQSg7tmE","executionInfo":{"status":"ok","timestamp":1763073406027,"user_tz":-420,"elapsed":1561,"user":{"displayName":"Tmabunda Hahabe","userId":"14655309613109040543"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["save_dir = \"/content/drive/MyDrive/Capstone4/YOLO_Backups\"\n","results = model.train(\n","    data=f'{dataset_location}/data.yaml',  # Path to dataset config\n","    epochs=100,\n","    batch=16,\n","    imgsz=640,\n","    project=save_dir,         # Save all results here\n","    name='run1',              # Folder name inside /yolo_backups (e.g. yolo_backups/run1)\n","    exist_ok=True,            # Allow overwriting existing run folder\n","    patience=5,               # Early stopping\n","    save_period=3,            # Save every 3 epochs\n","    val=True,                 # Validate during training\n","    verbose=True,             # Show details\n","    fliplr=0.5,               # Data augmentation (horizontal flip)\n","    scale=0.5,\n","    resume=True\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"suauF7dv5EUu","outputId":"9de2aa42-6a4b-4d27-f147-9ee60f94d2f8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Ultralytics 8.3.228 ğŸš€ Python-3.12.12 torch-2.8.0+cu126 CPU (Intel Xeon CPU @ 2.30GHz)\n","\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/drive/MyDrive/Capstone4/data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=100, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=/content/drive/MyDrive/Capstone4/YOLO_Backups/run1/weights/last.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=run1, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=5, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=/content/drive/MyDrive/Capstone4/YOLO_Backups, rect=False, resume=/content/drive/MyDrive/Capstone4/YOLO_Backups/run1/weights/last.pt, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/drive/MyDrive/Capstone4/YOLO_Backups/run1, save_frames=False, save_json=False, save_period=3, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.0, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=0, workspace=None\n","\u001b[KDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf': 100% â”â”â”â”â”â”â”â”â”â”â”â” 755.1KB 16.5MB/s 0.0s\n","\n","                   from  n    params  module                                       arguments                     \n","  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n","  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n","  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n","  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n","  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n","  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n","  6                  -1  2    180864  ultralytics.nn.modules.block.A2C2f           [128, 128, 2, True, 4]        \n","  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n","  8                  -1  2    689408  ultralytics.nn.modules.block.A2C2f           [256, 256, 2, True, 1]        \n","  9                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 10             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 11                  -1  1     86912  ultralytics.nn.modules.block.A2C2f           [384, 128, 1, False, -1]      \n"," 12                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 13             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 14                  -1  1     24000  ultralytics.nn.modules.block.A2C2f           [256, 64, 1, False, -1]       \n"," 15                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n"," 16            [-1, 11]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 17                  -1  1     74624  ultralytics.nn.modules.block.A2C2f           [192, 128, 1, False, -1]      \n"," 18                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n"," 19             [-1, 8]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 20                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n"," 21        [14, 17, 20]  1    431257  ultralytics.nn.modules.head.Detect           [3, [64, 128, 256]]           \n","YOLOv12n summary: 272 layers, 2,568,633 parameters, 2,568,617 gradients, 6.5 GFLOPs\n","\n","Transferred 691/691 items from pretrained weights\n","Freezing layer 'model.21.dfl.conv.weight'\n","\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 167.0Â±24.9 ms, read: 0.1Â±0.0 MB/s, size: 45.9 KB)\n","\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Capstone4/train/labels.cache... 9218 images, 15 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 9218/9218 11.3Mit/s 0.0s\n","WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 84, len(boxes) = 18494. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.8Â±0.4 ms, read: 0.1Â±0.1 MB/s, size: 28.5 KB)\n","\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Capstone4/valid/labels.cache... 287 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 287/287 367.2Kit/s 0.0s\n","WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 19, len(boxes) = 427. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n","Plotting labels to /content/drive/MyDrive/Capstone4/YOLO_Backups/run1/labels.jpg... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01, momentum=0.9) with parameter groups 113 weight(decay=0.0), 120 weight(decay=0.0005), 119 bias(decay=0.0)\n","Resuming training /content/drive/MyDrive/Capstone4/YOLO_Backups/run1/weights/last.pt from epoch 28 to 100 total epochs\n","Image sizes 640 train, 640 val\n","Using 0 dataloader workers\n","Logging results to \u001b[1m/content/drive/MyDrive/Capstone4/YOLO_Backups/run1\u001b[0m\n","Starting training for 100 epochs...\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K     28/100         0G      0.549     0.4851     0.9943         67        640: 12% â”â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 70/577 24.5s/it 29:37<3:27:01"]}]},{"cell_type":"markdown","source":["## Evaluation"],"metadata":{"id":"Yaa8CLplk55R"}},{"cell_type":"markdown","source":["### Step 9: Post-training Setup\n","\n","Fix for character encoding and display directory structure for output artifacts."],"metadata":{"id":"hfNGwyslSOb3"}},{"cell_type":"code","source":["locale.getpreferredencoding = lambda: \"UTF-8\"\n","run_name = \"run1\"\n","\n","# List everything YOLO saved in Drive\n","!ls {save_dir}/{run_name}/"],"metadata":{"id":"2EsxeLI9k4vM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762949636051,"user_tz":-420,"elapsed":111,"user":{"displayName":"Tmabunda Hahabe","userId":"14655309613109040543"}},"outputId":"a2a53154-c2a7-49f8-c1b5-1bf018f228ad"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["args.yaml   results.csv       train_batch1.jpg\tweights\n","labels.jpg  train_batch0.jpg  train_batch2.jpg\n"]}]},{"cell_type":"markdown","source":["### Step 10: Visualize Training Results\n","\n","Display the confusion matrix and training result curves from the YOLO training run. The confusion matrix: how well the model distinguishes between classes."],"metadata":{"id":"BA-fjxPaSTVW"}},{"cell_type":"code","source":["# Display the confusion matrix from your Google Drive folder\n","Image(filename=f'{save_dir}/{run_name}/confusion_matrix.png', width=1000)"],"metadata":{"id":"vlwnAURgiAgE","colab":{"base_uri":"https://localhost:8080/","height":304},"executionInfo":{"status":"error","timestamp":1762949639427,"user_tz":-420,"elapsed":36,"user":{"displayName":"Tmabunda Hahabe","userId":"14655309613109040543"}},"outputId":"61bfcd25-1119-479b-e72a-7f2934317b08"},"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/drive/MyDrive/Capstone4/YOLO_Backups/run1/confusion_matrix.png'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3155368968.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Display the confusion matrix from your Google Drive folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'{save_dir}/{run_name}/confusion_matrix.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename, format, embed, width, height, retina, unconfined, metadata)\u001b[0m\n\u001b[1;32m   1229\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretina\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretina\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1230\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munconfined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munconfined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1231\u001b[0;31m         super(Image, self).__init__(data=data, url=url, filename=filename, \n\u001b[0m\u001b[1;32m   1232\u001b[0m                 metadata=metadata)\n\u001b[1;32m   1233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename, metadata)\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1261\u001b[0m         \u001b[0;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1263\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1264\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretina\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retina_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    660\u001b[0m         \u001b[0;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_flags\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    663\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Capstone4/YOLO_Backups/run1/confusion_matrix.png'"]}]},{"cell_type":"markdown","source":["The training graph: loss, mAP, precision/recall over time."],"metadata":{"id":"XXe2FjJyYYpV"}},{"cell_type":"code","source":["Image(filename=f'{save_dir}/{run_name}/results.png', width=1000)"],"metadata":{"id":"Tk-9VnlviDUz","colab":{"base_uri":"https://localhost:8080/","height":287},"executionInfo":{"status":"error","timestamp":1762949697054,"user_tz":-420,"elapsed":73,"user":{"displayName":"Tmabunda Hahabe","userId":"14655309613109040543"}},"outputId":"eb09655f-9dd4-4b40-940a-aac87aa56840"},"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/drive/MyDrive/Capstone4/YOLO_Backups/run1/results.png'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2233085105.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'{save_dir}/{run_name}/results.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename, format, embed, width, height, retina, unconfined, metadata)\u001b[0m\n\u001b[1;32m   1229\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretina\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretina\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1230\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munconfined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munconfined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1231\u001b[0;31m         super(Image, self).__init__(data=data, url=url, filename=filename, \n\u001b[0m\u001b[1;32m   1232\u001b[0m                 metadata=metadata)\n\u001b[1;32m   1233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename, metadata)\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1261\u001b[0m         \u001b[0;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1263\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1264\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretina\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retina_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    660\u001b[0m         \u001b[0;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_flags\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    663\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Capstone4/YOLO_Backups/run1/results.png'"]}]},{"cell_type":"markdown","source":["### Step 11: Load Test Dataset for Evaluation\n","\n","Prepare test data using Supervision's dataset loader to evaluate model performance."],"metadata":{"id":"PBd-t9NASYM1"}},{"cell_type":"code","source":["ds = sv.DetectionDataset.from_yolo(\n","    images_directory_path=f\"{dataset_location}/test/images\",\n","    annotations_directory_path=f\"{dataset_location}/test/labels\",\n","    data_yaml_path=f\"{dataset_location}/data.yaml\"\n",")\n","ds.classes"],"metadata":{"id":"jW8QjS6tiI0Y","colab":{"base_uri":"https://localhost:8080/","height":269},"executionInfo":{"status":"error","timestamp":1762949793726,"user_tz":-420,"elapsed":66842,"user":{"displayName":"Tmabunda Hahabe","userId":"14655309613109040543"}},"outputId":"cd1851a0-e621-41a3-b73e-b9be4bdfbde2"},"execution_count":null,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3335107125.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m ds = sv.DetectionDataset.from_yolo(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mimages_directory_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"{dataset_location}/test/images\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mannotations_directory_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"{dataset_location}/test/labels\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdata_yaml_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"{dataset_location}/data.yaml\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/supervision/dataset/core.py\u001b[0m in \u001b[0;36mfrom_yolo\u001b[0;34m(cls, images_directory_path, annotations_directory_path, data_yaml_path, force_masks, is_obb)\u001b[0m\n\u001b[1;32m    462\u001b[0m             \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m         \"\"\"\n\u001b[0;32m--> 464\u001b[0;31m         classes, image_paths, annotations = load_yolo_annotations(\n\u001b[0m\u001b[1;32m    465\u001b[0m             \u001b[0mimages_directory_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimages_directory_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0mannotations_directory_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mannotations_directory_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/supervision/dataset/formats/yolo.py\u001b[0m in \u001b[0;36mload_yolo_annotations\u001b[0;34m(images_directory_path, annotations_directory_path, data_yaml_path, force_masks, is_obb)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;31m# PIL is much faster than cv2 for checking image shape and mode: https://github.com/roboflow/supervision/issues/1554\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_txt_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mannotation_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_empty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0mresolution_wh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/supervision/utils/file.py\u001b[0m in \u001b[0;36mread_txt_file\u001b[0;34m(file_path, skip_empty)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mskip_empty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","source":["### Step 12: Compute Mean Average Precision\n","\n","Evaluate the trained model using mAP metrics (50, 75, and 50:95)."],"metadata":{"id":"D0eG8FyvSeQ4"}},{"cell_type":"code","source":["# === Load your trained YOLO model from Google Drive ===\n","model = YOLO(f\"{save_dir}/{run_name}/weights/best.pt\")\n","\n","# === Initialize lists to collect predictions and ground truths ===\n","predictions = []\n","targets = []\n","\n","for _, image, target in ds:\n","    results = model(image, verbose=False)[0]\n","    detections = sv.Detections.from_ultralytics(results)\n","\n","    predictions.append(detections)\n","    targets.append(target)\n","\n","# === Compute Mean Average Precision ===\n","map = MeanAveragePrecision().update(predictions, targets).compute()"],"metadata":{"id":"gU72wiTkiK-d","colab":{"base_uri":"https://localhost:8080/","height":211},"executionInfo":{"status":"error","timestamp":1762949800679,"user_tz":-420,"elapsed":4767,"user":{"displayName":"Tmabunda Hahabe","userId":"14655309613109040543"}},"outputId":"e12e6d37-ad4c-4034-ebc7-9ec5e3c13ebc"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'ds' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-4197453069.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mdetections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDetections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_ultralytics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'ds' is not defined"]}]},{"cell_type":"markdown","source":["The mAP metrics are printed and the results are plotted."],"metadata":{"id":"_OKZmL7ySjMc"}},{"cell_type":"code","source":["print(\"mAP 50:95\", map.map50_95)\n","print(\"mAP 50\", map.map50)\n","print(\"mAP 75\", map.map75)"],"metadata":{"id":"Wc3zaXvhiNU7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["map.plot()"],"metadata":{"id":"5dCyxSPDiPIa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Step 13: Inference on Random Test Image\n","\n","Run the trained model on a random test image and visualize detections."],"metadata":{"id":"c0IJB2GwSnow"}},{"cell_type":"markdown","source":["- Picks a random image.\n","- Runs the trained YOLOv12 model.\n","- Applies NMS (Non-Max Suppression) to clean overlapping boxes."],"metadata":{"id":"ApV5rDjlYlMq"}},{"cell_type":"code","source":["import random\n","\n","i = random.randint(0, len(ds))\n","\n","image_path, image, target = ds[i]\n","\n","results = model(image, verbose=False)[0]\n","detections = sv.Detections.from_ultralytics(results).with_nms()\n","\n","box_annotator = sv.BoxAnnotator()\n","label_annotator = sv.LabelAnnotator()\n","\n","annotated_image = image.copy()\n","annotated_image = box_annotator.annotate(scene=annotated_image, detections=detections)\n","annotated_image = label_annotator.annotate(scene=annotated_image, detections=detections)"],"metadata":{"id":"X5TFx1CnYrJB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["`plot_image` displays the final annotated image with predicted bounding boxes and labels."],"metadata":{"id":"4NQm6DPhSt94"}},{"cell_type":"code","source":["sv.plot_image(annotated_image)"],"metadata":{"id":"62gVpqL_iUs1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ğŸ” **Conclusion**\n","\n","Over the course of 20 training epochs using YOLOv12n, the model showed consistent improvements in all performance metrics, including precision (P), recall (R), mean average precision at IoU=0.50 (mAP@50), and mean average precision across IoU thresholds (mAP@50-95).\n","\n","#### ğŸ“ˆ Performance Summary:\n","- **Initial performance** (Epoch 1):  \n","  - mAP@50 = **0.396**, mAP@50-95 = **0.211**\n","- **Final performance** (Epoch 20):  \n","  - mAP@50 = **0.945**, mAP@50-95 = **0.749**\n","- **Validation with best.pt**:\n","  - Overall mAP@50 = **0.945**\n","  - Class-wise mAP@50:\n","    - Paper: **0.953**\n","    - Rock: **0.939**\n","    - Scissors: **0.942**\n","\n","These results indicate a successful training process with high detection accuracy across all classes.\n","\n","#### âœ… Problem Solved:\n","This hands-on training addressed the problem of detecting three object categories â€” **Paper**, **Rock**, and **Scissors** â€” in images. By fine-tuning the YOLOv12n model, we achieved strong object detection performance on a moderately sized dataset (576 images, 400 instances).\n","\n","The results demonstrate YOLOv12n's efficiency and capability in solving real-world object detection tasks with relatively fast training time (~0.77 hours) and low model size (5.5MB).\n","\n","This experiment validates YOLOv12n as a suitable choice for lightweight, accurate object detection in constrained environments."],"metadata":{"id":"LVTsVbQcUBOF"}}]}